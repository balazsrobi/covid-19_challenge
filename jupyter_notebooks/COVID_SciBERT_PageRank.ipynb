{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approach comes from [here]()\n",
    "\n",
    "The only difference is, that I propose to substitute USE for SciBERT.\n",
    "\n",
    "SciBERT can be accessed [here](https://github.com/allenai/scibert) via the [Huggingface transformers](https://github.com/huggingface/transformers) package.\n",
    "\n",
    "Please investigate usage, I could not go into detail yet, sorry!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use SciBERT to vectorize text\n",
    "\n",
    "__WARNING!__\n",
    "\n",
    "\n",
    "Have to figure out if this last_hidden_states is a vector for a sentence a token or what. \n",
    "\n",
    "\n",
    "Requires checking the docs, I DID NOT TEST THIS!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "model = AutoModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "\n",
    "# Please check this usage, not sure it is ok!!!!!\n",
    "input_ids = torch.tensor([tokenizer.encode(\"Here is some text to encode\", add_special_tokens=True)])  # Add special tokens takes care of adding [CLS], [SEP], <s>... tokens in the right way for each model.\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids)[0]\n",
    "\n",
    "# WARNING!\n",
    "# Have to figure out if this last_hidden_states is a vector for a sentence a token or what. \n",
    "# Requires checking the docs, I DID NOT TEST THIS!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If figured out, put the sentence vectors into sentence_vectors as numpy\n",
    "sentence_vectors = embed(sentenced_text).numpy() #WARNING, this is a different approach, just illustration from a different vectorizer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing the distance matrix\n",
    "\n",
    "If we have our nice sentence vectors, it is time to do a similarity / distance matrix from them, which we will feed to PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!pip install networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "distance_matrix = cosine_distances(sentence_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df= pd.DataFrame(distance_matrix) #.stack()\n",
    "\n",
    "stacked_df = df.stack()\n",
    "stacked_df = stacked_df.rename_axis(('source', 'target')).reset_index(name='weight')\n",
    "stacked_df[\"weight\"]=stacked_df[\"weight\"]*-1.0\n",
    "stacked_df[\"weight\"]=stacked_df[\"weight\"]+2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating centrality\n",
    "\n",
    "Now we have a nice distance matrix, time to run PageRank and see, which are the most \"central\" elements!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "G = nx.from_pandas_edgelist(stacked_df,  edge_attr=True)\n",
    "print (nx.to_dict_of_dicts(G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = nx.pagerank(G, weight=\"weight\", alpha=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative approach\n",
    "\n",
    "We take the sentence vectors, and use them for approximate nearest neighbor searches, with the tool [Annoy](https://github.com/spotify/annoy). Basically this means, that if we can vectorize a query, we can use annoy to look up $n$ nearest neighbors, thus, we have a search engine.\n",
    "\n",
    "Moreover, if we take the average vector of the document, use that as a query, we can get the most \"important\" sentences, that, which are the closest in meaning with the document itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user annoy --upgrade --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: demo code below, the variable names are totally not the same, just copypasted..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex\n",
    "\n",
    "f = 768\n",
    "t = AnnoyIndex(f, 'angular') \n",
    "\n",
    "for i in range(len(all_sentence_vectors)):\n",
    "    t.add_item(i, all_sentence_vectors[i])\n",
    "\n",
    "t.build(10)\n",
    "\n",
    "nearest_sentence_indices = t.get_nns_by_vector(doc.vector/doc.vector_norm, 5)\n",
    "sentences = list(doc.sents)\n",
    "\n",
    "for i in nearest_sentence_indices:\n",
    "    print(sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WARNING: it is to be investigated, hogy SciBERT works, if the same as BERT, \n",
    "#then ALL the vectors are to be normalized!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
